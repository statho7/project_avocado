{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# !pip install --upgrade pandas\n",
    "# !pip install prophet\n",
    "# !pip install -U numpy\n",
    "# !pip uninstall pillow\n",
    "# !pip install pillow\n",
    "# !pip install -U scipy\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn matplotlib\n",
    "# !pip install statsmodels\n",
    "# !pip install sklearn\n",
    "\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import make_interp_spline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "from column import Column\n",
    "from region import Region\n",
    "from util import plotRegionObservations, findRegionsByNumberObservations, findRegionsByName, boxplotsOfDataframe, histogramsOfDataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info about our dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the dataset we are going to examine we have data downloaded from the Hass Avocado Board website in May of 2018. The data represents weekly retail scanned data for National retail volume (units) and price between 2015 and 2018.\n",
    "\n",
    "The columns of the csv file are the following:\n",
    "\n",
    "- Unnamed column that represents the number of weeks left for the end of the year of the observation\n",
    "- Date - The date of the observation\n",
    "- AveragePrice - the average price of a single avocado\n",
    "- Total Volume - Total number of avocados sold\n",
    "- 4046 - Total number of avocados with PLU 4046 sold\n",
    "- 4225 - Total number of avocados with PLU 4225 sold\n",
    "- 4770 - Total number of avocados with PLU 4770 sold\n",
    "- Total Bags - Total number of bags of avocados sold\n",
    "- Small Bags - Total number of small bags of avocados sold\n",
    "- Large Bags - Total number of large bags of avocados sold\n",
    "- XLarge Bags - Total number of xlarge bags of avocados sold\n",
    "- type - conventional or organic\n",
    "- year - the year of the observation\n",
    "- region - the city or region of the observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "avocado_df = pd.read_csv('./data/avocado.csv')\n",
    "\n",
    "avocado_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete index column\n",
    "\n",
    "As the unnmaed column that represents the number of weeks left for the end of the year of the observation does not seem of use we will remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocado_df = avocado_df.drop(list(avocado_df)[0], axis=1)\n",
    "\n",
    "avocado_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regions\n",
    "\n",
    "We will examine first the regions column, its distribution and an interesting fact that we need to consider in our analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = avocado_df['region'].value_counts()\n",
    "\n",
    "print(\"Number of regions on the dataset are\",len(regions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see on the next histogram most of the regions/cities of our dataset have 338 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotRegionObservations(regions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see below the only region with 335 observations is WestTexNewMexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findRegionsByNumberObservations(regions, 335)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important facts we need to consider about this dataset though is that there is a region named TotalUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findRegionsByName(regions, \"TotalUS\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means in order to examine the whole US and then the weight for each region we should not work on the whole dataset and the each different region.\n",
    "\n",
    "We need to filter the dataframe and work with the observations that have the value TotalUS as region to understand what happens to US as a whole.\n",
    "\n",
    "Of course once we need to examine each region on its own or compared to whole US we need to take into account for the region WestTexNewMexico that it has 3 less observations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics for Total US"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions of columns\n",
    "\n",
    "We will calculate summary stats like mean, median, min, max, quantiles, etc. for each column to understand distributions. After that the next step will be to use boxplots and histograms as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing that we will do a check to see the type of data each column holds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_us_df = avocado_df[avocado_df[\"region\"]==\"TotalUS\"]\n",
    "\n",
    "#Types of columns\n",
    "print(total_us_df.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As not every column holds numeric data (float or int) we will focus on the numeric ones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "print(total_us_df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplotsOfDataframe(total_us_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_us_df = total_us_df.sort_values(by='Date')\n",
    "total_us_df.reset_index(drop=True)\n",
    "\n",
    "total_us_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogramsOfDataframe(total_us_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze trends over time - create line charts for average price, volume, bags sold, etc. over the date range to visually inspect trends and seasonality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Price trends"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine the trends for average price we are going to plot a graph for each year and then try to compare the years to see obvious patterns based on seasonality.\n",
    "\n",
    "First step is to create a new dataframe that will combine the duplicate rows for the same date as the current dataset has one row for each date, one for the organic avocados observation and the other for the conventional. Then this new dataframe will be used to plot our graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame to have 'Date' as the index and 'Type' as columns\n",
    "pivot_df = total_us_df.pivot(index=['Date','year'], columns='type', values='AveragePrice')\n",
    "\n",
    "# Calculate the average of 'Organic' and 'Conventional' columns\n",
    "pivot_df['AveragePriceCombined'] = pivot_df.mean(axis=1)\n",
    "\n",
    "# Reset the index to have 'Date' as a regular column\n",
    "result_df = pivot_df.reset_index()\n",
    "\n",
    "# Optionally, you can rename the columns as needed\n",
    "result_df.rename(columns={'Date': 'Date', 'AveragePriceCombined': 'AveragePrice','organic':'Organic','conventional':'Conventional'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs will have 3 lines. One line will be the average price of the organic avocados through the year, the other for the conventional and the final one will be the combination of the first two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_df = result_df.loc[:]\n",
    "# [result_df['year']==year]\n",
    "years_df.reset_index()\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "for year in range(2015,2017):\n",
    "    plt.plot(years_df[years_df['year']==year]['Date'], years_df[years_df['year']==year]['Organic'], label=f'Organic_{year}')\n",
    "    plt.plot(years_df[years_df['year']==year]['Date'], years_df[years_df['year']==year]['Conventional'], label=f'Conventional_{year}')\n",
    "    # plt.plot(year_df['Date'], year_df['AveragePrice'], label='Average Price', linestyle='--')\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Price')\n",
    "plt.title('Price Trend for Conventional and Organic avocados')\n",
    "plt.legend()\n",
    "\n",
    "# Rotate x-axis labels for better readability (optional)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_df[years_df['year']==2015]['Date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_df = result_df.loc[:]\n",
    "years_df['Date'] = pd.to_datetime(years_df['Date'])\n",
    "years_df['Date'] = years_df['Date'].dt.month_name() + ' ' + years_df['Date'].dt.day.astype(str)\n",
    "# years_df['Date'] = pd.to_datetime(years_df['Date'])\n",
    "years_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_df = result_df.loc[:]\n",
    "years_df.reset_index()\n",
    "\n",
    "years_df['Date'] = pd.to_datetime(years_df['Date'])\n",
    "years_df['Date'] = years_df['Date'].dt.month_name() + ' ' + years_df['Date'].dt.day.astype(str)\n",
    "# years_df['Date'] = pd.to_datetime(years_df['Date'])\n",
    "# [result_df['year']==year]\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "for year in range(2015,2018):\n",
    "    plt.plot(years_df[years_df['year']==year]['Date'], years_df[years_df['year']==year]['Organic'], label=f'Organic_{year}')\n",
    "    plt.plot(years_df[years_df['year']==year]['Date'], years_df[years_df['year']==year]['Conventional'], label=f'Conventional_{year}')\n",
    "    # plt.plot(year_df['Date'], year_df['AveragePrice'], label='Average Price', linestyle='--')\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Price')\n",
    "plt.title('Price Trend for Conventional and Organic avocados')\n",
    "plt.legend()\n",
    "\n",
    "# Rotate x-axis labels for better readability (optional)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily price difference  \n",
    "result_df['PriceDiffOrganic'] = result_df['Organic'].diff()\n",
    "result_df['PriceDiffConventional'] = result_df['Conventional'].diff()\n",
    "\n",
    "# Calculate percentage difference\n",
    "result_df['PriceDiffPercentOrganic'] = result_df['PriceDiffOrganic']/result_df['Organic'].shift(1) * 100\n",
    "result_df['PriceDiffPercentConventional'] = result_df['PriceDiffConventional']/result_df['Conventional'].shift(1) * 100\n",
    "\n",
    "# # Plot average price over date\n",
    "plt.figure(figsize=(24,8))\n",
    "# Plot trends\n",
    "# plt.plot(result_df['Date'], result_df['PriceDiff'], label='Daily Difference')\n",
    "plt.plot(result_df['Date'], result_df['PriceDiffPercentOrganic'], label='Organic Daily % Change')\n",
    "plt.plot(result_df['Date'], result_df['PriceDiffPercentConventional'], label='Conventional Daily % Change')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('USD Price Change')\n",
    "plt.title('Avocado Price Trends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily price difference  \n",
    "result_df['PriceDiffOrganic'] = result_df['Organic'].diff()\n",
    "result_df['PriceDiffConventional'] = result_df['Conventional'].diff()\n",
    "\n",
    "# # Calculate percentage difference\n",
    "# result_df['PriceDiffPercentOrganic'] = result_df['PriceDiffOrganic']/result_df['Organic'].shift(1) * 100\n",
    "# result_df['PriceDiffPercentConventional'] = result_df['PriceDiffConventional']/result_df['Conventional'].shift(1) * 100\n",
    "\n",
    "# # Plot average price over date\n",
    "plt.figure(figsize=(24,8))\n",
    "# Plot trends\n",
    "# plt.plot(result_df['Date'], result_df['PriceDiff'], label='Daily Difference')\n",
    "plt.plot(result_df['Date'], result_df['PriceDiffOrganic'], label='Organic Daily % Change')\n",
    "plt.plot(result_df['Date'], result_df['PriceDiffConventional'], label='Conventional Daily % Change')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('USD Price Change')\n",
    "plt.title('Avocado Price Trends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years_df = result_df.loc[:]\n",
    "# [result_df['year']==year]\n",
    "years_df.reset_index()\n",
    "\n",
    "# Create a line plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "for year in range(2015,2017):\n",
    "    plt.plot(years_df[years_df['year']==year]['Date'], years_df[years_df['year']==year]['Organic'], label=f'Organic_{year}')\n",
    "    plt.plot(years_df[years_df['year']==year]['Date'], years_df[years_df['year']==year]['Conventional'], label=f'Conventional_{year}')\n",
    "    # plt.plot(year_df['Date'], year_df['AveragePrice'], label='Average Price', linestyle='--')\n",
    "\n",
    "# Add labels and a legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Price')\n",
    "plt.title('Price Trend for Conventional and Organic avocados')\n",
    "plt.legend()\n",
    "\n",
    "# Rotate x-axis labels for better readability (optional)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2015,2018):\n",
    "    year_df = result_df[result_df['year']==year].loc[:]\n",
    "    \n",
    "    year_df.reset_index()\n",
    "    # Create a line plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(year_df['Date'], year_df['Organic'], label='Organic')\n",
    "    plt.plot(year_df['Date'], year_df['Conventional'], label='Conventional')\n",
    "    # plt.plot(year_df['Date'], year_df['AveragePrice'], label='Average Price', linestyle='--')\n",
    "\n",
    "    # Add labels and a legend\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Average Price')\n",
    "    plt.title('Price Trend for Conventional and Organic avocados')\n",
    "    plt.legend()\n",
    "\n",
    "    # Rotate x-axis labels for better readability (optional)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first observations from the graphs above is that it seems there is a peak between August and October for the prices of Avocados for both the organic and conventional types. From October and till the end of the year there is a decline on the Average Price.\n",
    "\n",
    "To understand if this decline is driven by less demand we will plot the graphs for Total Volume of avocados being sold in the whole US like we just did for the Average Price."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total volume trends"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the difference between volume of conventional and organic is big we will plot them in different graphs to avoid seeing a flat line for the total volume of organic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame to have 'Date' as the index and 'Type' as columns\n",
    "pivot_df = total_us_df.pivot(index=['Date','year'], columns='type', values='Total Volume')\n",
    "\n",
    "# Calculate the average of 'Organic' and 'Conventional' columns\n",
    "pivot_df['TotalVolumeCombined'] = pivot_df.mean(axis=1)\n",
    "\n",
    "# Reset the index to have 'Date' as a regular column\n",
    "result_df = pivot_df.reset_index()\n",
    "\n",
    "# Optionally, you can rename the columns as needed\n",
    "result_df.rename(columns={'Date': 'Date', 'TotalVolumeCombined': 'TotalVolume','organic':'Organic','conventional':'Conventional'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2015,2018):\n",
    "    ## Organic\n",
    "    year_df = result_df[result_df['year']==year].loc[:]\n",
    "\n",
    "    year_df.reset_index()\n",
    "    # Create a line plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(year_df['Date'], year_df['Organic'], label='Organic')\n",
    "    # plt.plot(year_df['Date'], year_df['Conventional'], label='Conventional')\n",
    "    # plt.plot(year_df['Date'], year_df['TotalVolume'], label='Total Volume', linestyle='--')\n",
    "\n",
    "    # Add labels and a legend\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total Volume')\n",
    "    plt.title(f'Total volume trend of Organic avocados for year {year}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Rotate x-axis labels for better readability (optional)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    ## Conventional\n",
    "    year_df = result_df[result_df['year']==year].loc[:]\n",
    "\n",
    "    year_df.reset_index()\n",
    "    # Create a line plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    # plt.plot(year_df['Date'], year_df['Organic'], label='Organic')\n",
    "    plt.plot(year_df['Date'], year_df['Conventional'], label='Conventional')\n",
    "    # plt.plot(year_df['Date'], year_df['TotalVolume'], label='Total Volume', linestyle='--')\n",
    "\n",
    "    # Add labels and a legend\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total Volume')\n",
    "    plt.title(f'Total volume trend of Conventional avocados for year {year}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Rotate x-axis labels for better readability (optional)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis - calculate correlation coefficients between variables to identify relationships, like price and volume sold. Create a correlation matrix visualization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO normalize columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis for conventional avocados in US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conventional_total_us_df = total_us_df[total_us_df['type']=='conventional'].loc[:].reset_index(drop=True)\n",
    "\n",
    "# num_cols=[]\n",
    "# for col in conventional_total_us_df.columns:\n",
    "#     if pd.api.types.is_numeric_dtype(conventional_total_us_df[col]):\n",
    "#         num_cols.append(col)\n",
    "\n",
    "# corr_matrix_conventional_us=conventional_total_us_df[num_cols].corr()# Create a heatmap of the correlation matrix\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(corr_matrix_conventional_us, annot=True, cmap=sns.diverging_palette(1, 255, as_cmap=True), fmt=\".2f\")\n",
    "# plt.title('Correlation Matrix for whole US only on conventional avocados')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "numerical_cols = ['AveragePrice','Total Volume','4046','4225','4770','Total Bags','Small Bags','Large Bags','XLarge Bags'] \n",
    "df=total_us_df\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['year'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conventional_total_us_df = df[df['type']=='conventional'].loc[:].reset_index(drop=True)\n",
    "conventional_total_us_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conventional_total_us_df234 = df[df['type']=='conventional'].loc[:].reset_index(drop=True)\n",
    "\n",
    "num_cols=[]\n",
    "for col in conventional_total_us_df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(conventional_total_us_df[col]):\n",
    "        num_cols.append(col)\n",
    "\n",
    "corr_matrix_conventional_us1=conventional_total_us_df234[num_cols].corr()# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix_conventional_us1, annot=True, cmap=sns.diverging_palette(1, 255, as_cmap=True), fmt=\".2f\")\n",
    "plt.title('Correlation Matrix for whole US only on conventional avocados')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conventional_total_us_df = total_us_df[total_us_df['type']=='conventional'].loc[:].reset_index(drop=True)\n",
    "\n",
    "num_cols=[]\n",
    "for col in conventional_total_us_df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(conventional_total_us_df[col]):\n",
    "        num_cols.append(col)\n",
    "\n",
    "corr_matrix_conventional_us=conventional_total_us_df[num_cols].corr()# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix_conventional_us, annot=True, cmap=sns.diverging_palette(1, 255, as_cmap=True), fmt=\".2f\")\n",
    "plt.title('Correlation Matrix for whole US only on conventional avocados')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix=corr_matrix_conventional_us\n",
    "\n",
    "# Replace diagonal elements (correlation of 1) with NaN to exclude them\n",
    "np.fill_diagonal(correlation_matrix.values, np.nan)\n",
    "\n",
    "# Flatten the correlation matrix\n",
    "correlation_values = correlation_matrix.values.flatten()\n",
    "\n",
    "# Sort the correlation values in ascending order and get the indices\n",
    "sorted_indices = np.argsort(correlation_values)\n",
    "\n",
    "# Get the 5 lowest correlations\n",
    "lowest_corr_indices = sorted_indices[:5]\n",
    "lowest_corr_values = correlation_values[lowest_corr_indices]\n",
    "lowest_corr_vars = [\n",
    "    (\n",
    "        correlation_matrix.columns[i // correlation_matrix.shape[0]],\n",
    "        correlation_matrix.columns[i % correlation_matrix.shape[0]]\n",
    "    )\n",
    "    for i in lowest_corr_indices\n",
    "]\n",
    "\n",
    "# Print the 5 lowest and 5 highest correlations\n",
    "print(\"\\n5 Lowest Correlations:\")\n",
    "for i, (var1, var2) in enumerate(lowest_corr_vars):\n",
    "    print(f\"{i + 1}. Variables: {var1} and {var2}, Correlation Value: {correlation_values[lowest_corr_indices[i]]:.2f}\")\n",
    "\n",
    "# Replace NaN values with a very low value (e.g., -2) to exclude them\n",
    "correlation_matrix = correlation_matrix.fillna(-2)\n",
    "\n",
    "# Find the indices of the 5 highest correlations (excluding NaN)\n",
    "highest_corr_indices = np.unravel_index(np.argsort(correlation_matrix.values, axis=None)[-5:], correlation_matrix.shape)\n",
    "highest_corr_values = correlation_matrix.values[highest_corr_indices]\n",
    "\n",
    "# Get the variable pairs for the 5 highest correlations\n",
    "highest_corr_vars = [\n",
    "    (\n",
    "        correlation_matrix.columns[highest_corr_indices[0][i]],\n",
    "        correlation_matrix.columns[highest_corr_indices[1][i]]\n",
    "    )\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "# Print the 5 highest correlations\n",
    "print(\"\\n5 Highest Correlations (Excluding NaN):\")\n",
    "for i, (var1, var2) in enumerate(highest_corr_vars):\n",
    "    print(f\"{i + 1}. Variables: {var1} and {var2}, Correlation Value: {highest_corr_values[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column1 = 'AveragePrice'\n",
    "\n",
    "# Extract the correlations of 'AveragePrice' with the rest of the columns\n",
    "correlations_with_column1 = correlation_matrix.loc[column1].drop(column1)\n",
    "\n",
    "# Get the top 5 highest correlations for 'AveragePrice'\n",
    "top_5_highest_corr_with_column1 = correlations_with_column1.nlargest(5)\n",
    "bottom_5_lowest_corr_with_column1 = correlations_with_column1.nsmallest(5)\n",
    "\n",
    "# Print the top 5 highest correlations for 'AveragePrice'\n",
    "print(f\"\\nTop 5 Highest Correlations with {column1}:\")\n",
    "print(top_5_highest_corr_with_column1)\n",
    "\n",
    "# Print the 5 lowest correlations for 'AveragePrice'\n",
    "print(f\"\\n5 Lowest Correlations with {column1}:\")\n",
    "print(bottom_5_lowest_corr_with_column1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column2 = 'Total Volume'\n",
    "\n",
    "# Extract the correlations of 'Total Volume' with the rest of the columns\n",
    "correlations_with_column2 = correlation_matrix.loc[column2].drop(column2)\n",
    "\n",
    "# Get the top 5 highest correlations for 'Total Volume'\n",
    "top_5_highest_corr_with_column2 = correlations_with_column2.nlargest(5)\n",
    "bottom_5_lowest_corr_with_column2 = correlations_with_column2.nsmallest(5)\n",
    "\n",
    "# Print the top 5 highest correlations for 'Total Volume'\n",
    "\n",
    "print(f\"\\nTop 5 Highest Correlations with {column2}:\")\n",
    "print(top_5_highest_corr_with_column2)\n",
    "\n",
    "# Print the 5 lowest correlations for 'Total Volume'\n",
    "print(f\"\\n5 Lowest Correlations with {column2}:\")\n",
    "print(bottom_5_lowest_corr_with_column2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix=corr_matrix_conventional_us\n",
    "\n",
    "# Replace NaN values with a very low value (e.g., -2) to exclude them\n",
    "correlation_matrix = correlation_matrix.fillna(-2)\n",
    "\n",
    "# Find the indices of the 5 highest correlations (excluding NaN)\n",
    "highest_corr_indices = np.unravel_index(np.argsort(correlation_matrix.values, axis=None)[-5:], correlation_matrix.shape)\n",
    "highest_corr_values = correlation_matrix.values[highest_corr_indices]\n",
    "\n",
    "# Get the variable pairs for the 5 highest correlations\n",
    "highest_corr_vars = [\n",
    "    (\n",
    "        correlation_matrix.columns[highest_corr_indices[0][i]],\n",
    "        correlation_matrix.columns[highest_corr_indices[1][i]]\n",
    "    )\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "# Print the 5 highest correlations\n",
    "print(\"5 Highest Correlations (Excluding NaN):\")\n",
    "for i, (var1, var2) in enumerate(highest_corr_vars):\n",
    "    print(f\"{i + 1}. Variables: {var1} and {var2}, Correlation Value: {highest_corr_values[i]:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the heatmap that we can see above there are some correlation that seem important to note.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis for organic avocados in US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organic_total_us_df = total_us_df[total_us_df['type']=='organic'].loc[:].reset_index(drop=True)\n",
    "\n",
    "num_cols=[]\n",
    "for col in organic_total_us_df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(organic_total_us_df[col]):\n",
    "        num_cols.append(col)\n",
    "\n",
    "corr_matrix_organic_us=organic_total_us_df[num_cols].corr()# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix_organic_us, annot=True, cmap=sns.diverging_palette(1, 255, as_cmap=True), fmt=\".2f\")\n",
    "plt.title('Correlation Matrix for whole US only on organic avocados')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix=corr_matrix_organic_us\n",
    "\n",
    "# Replace diagonal elements (correlation of 1) with NaN to exclude them\n",
    "np.fill_diagonal(correlation_matrix.values, np.nan)\n",
    "\n",
    "# Flatten the correlation matrix\n",
    "correlation_values = correlation_matrix.values.flatten()\n",
    "\n",
    "# Sort the correlation values in ascending order and get the indices\n",
    "sorted_indices = np.argsort(correlation_values)\n",
    "\n",
    "# Get the 5 lowest correlations\n",
    "lowest_corr_indices = sorted_indices[:5]\n",
    "lowest_corr_values = correlation_values[lowest_corr_indices]\n",
    "lowest_corr_vars = [\n",
    "    (\n",
    "        correlation_matrix.columns[i // correlation_matrix.shape[0]],\n",
    "        correlation_matrix.columns[i % correlation_matrix.shape[0]]\n",
    "    )\n",
    "    for i in lowest_corr_indices\n",
    "]\n",
    "\n",
    "# Print the 5 lowest and 5 highest correlations\n",
    "print(\"\\n5 Lowest Correlations:\")\n",
    "for i, (var1, var2) in enumerate(lowest_corr_vars):\n",
    "    print(f\"{i + 1}. Variables: {var1} and {var2}, Correlation Value: {correlation_values[lowest_corr_indices[i]]:.2f}\")\n",
    "\n",
    "# Replace NaN values with a very low value (e.g., -2) to exclude them\n",
    "correlation_matrix = correlation_matrix.fillna(-2)\n",
    "\n",
    "# Find the indices of the 5 highest correlations (excluding NaN)\n",
    "highest_corr_indices = np.unravel_index(np.argsort(correlation_matrix.values, axis=None)[-5:], correlation_matrix.shape)\n",
    "highest_corr_values = correlation_matrix.values[highest_corr_indices]\n",
    "\n",
    "# Get the variable pairs for the 5 highest correlations\n",
    "highest_corr_vars = [\n",
    "    (\n",
    "        correlation_matrix.columns[highest_corr_indices[0][i]],\n",
    "        correlation_matrix.columns[highest_corr_indices[1][i]]\n",
    "    )\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "# Print the 5 highest correlations\n",
    "print(\"\\n5 Highest Correlations (Excluding NaN):\")\n",
    "for i, (var1, var2) in enumerate(highest_corr_vars):\n",
    "    print(f\"{i + 1}. Variables: {var1} and {var2}, Correlation Value: {highest_corr_values[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column1 = 'AveragePrice'\n",
    "\n",
    "# Extract the correlations of 'AveragePrice' with the rest of the columns\n",
    "correlations_with_column1 = correlation_matrix.loc[column1].drop(column1)\n",
    "\n",
    "# Get the top 5 highest correlations for 'AveragePrice'\n",
    "top_5_highest_corr_with_column1 = correlations_with_column1.nlargest(5)\n",
    "bottom_5_lowest_corr_with_column1 = correlations_with_column1.nsmallest(5)\n",
    "\n",
    "# Print the top 5 highest correlations for 'AveragePrice'\n",
    "print(f\"\\nTop 5 Highest Correlations with {column1}:\")\n",
    "print(top_5_highest_corr_with_column1)\n",
    "\n",
    "# Print the 5 lowest correlations for 'AveragePrice'\n",
    "print(f\"\\n5 Lowest Correlations with {column1}:\")\n",
    "print(bottom_5_lowest_corr_with_column1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column2 = 'Total Volume'\n",
    "\n",
    "# Extract the correlations of 'Total Volume' with the rest of the columns\n",
    "correlations_with_column2 = correlation_matrix.loc[column2].drop(column2)\n",
    "\n",
    "# Get the top 5 highest correlations for 'Total Volume'\n",
    "top_5_highest_corr_with_column2 = correlations_with_column2.nlargest(5)\n",
    "bottom_5_lowest_corr_with_column2 = correlations_with_column2.nsmallest(5)\n",
    "\n",
    "# Print the top 5 highest correlations for 'Total Volume'\n",
    "\n",
    "print(f\"\\nTop 5 Highest Correlations with {column2}:\")\n",
    "print(top_5_highest_corr_with_column2)\n",
    "\n",
    "# Print the 5 lowest correlations for 'Total Volume'\n",
    "print(f\"\\n5 Lowest Correlations with {column2}:\")\n",
    "print(bottom_5_lowest_corr_with_column2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify outliers - use statistical methods or visual inspection to find outlier points that could indicate errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventional avocados in whole US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = conventional_total_us_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate the Z-scores for the numeric columns\n",
    "z_scores = np.abs(stats.zscore(numeric_columns))\n",
    "\n",
    "# Define the threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# Create a mask to identify outlier rows\n",
    "outlier_mask = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "# Get the rows containing outliers\n",
    "outliers = conventional_total_us_df[outlier_mask]\n",
    "\n",
    "# Print or further analyze the rows containing outliers\n",
    "print(\"Rows containing outliers:\")\n",
    "outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_check = 'AveragePrice'\n",
    "\n",
    "# Calculate the Z-scores for the selected columns\n",
    "z_scores = np.abs(stats.zscore(conventional_total_us_df[column_to_check]))\n",
    "\n",
    "# Define the threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# Create a mask to identify outlier rows\n",
    "outlier_mask = z_scores > threshold\n",
    "\n",
    "# Get the rows containing outliers in the selected column\n",
    "outliers = conventional_total_us_df[outlier_mask]\n",
    "\n",
    "# Print or further analyze the rows containing outliers\n",
    "print(f\"Rows containing outliers in '{column_to_check}':\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = np.percentile(conventional_total_us_df[column_to_check], 25)\n",
    "Q3 = np.percentile(conventional_total_us_df[column_to_check], 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for identifying outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame to get rows with outliers in the selected column\n",
    "outliers = conventional_total_us_df[(conventional_total_us_df[column_to_check] < lower_bound) | (conventional_total_us_df[column_to_check] > upper_bound)]\n",
    "\n",
    "# Print or further analyze the rows containing outliers\n",
    "print(f\"Rows containing outliers in '{column_to_check}':\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=conventional_total_us_df[column_to_check])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_check = 'Total Volume'\n",
    "\n",
    "# Calculate the Z-scores for the selected columns\n",
    "z_scores = np.abs(stats.zscore(conventional_total_us_df[column_to_check]))\n",
    "\n",
    "# Define the threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# Create a mask to identify outlier rows\n",
    "outlier_mask = z_scores > threshold\n",
    "\n",
    "# Get the rows containing outliers in the selected column\n",
    "outliers = conventional_total_us_df[outlier_mask]\n",
    "\n",
    "# Print or further analyze the rows containing outliers\n",
    "print(f\"Rows containing outliers in '{column_to_check}':\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = np.percentile(conventional_total_us_df[column_to_check], 25)\n",
    "Q3 = np.percentile(conventional_total_us_df[column_to_check], 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for identifying outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame to get rows with outliers in the selected column\n",
    "outliers = conventional_total_us_df[(conventional_total_us_df[column_to_check] < lower_bound) | (conventional_total_us_df[column_to_check] > upper_bound)]\n",
    "\n",
    "# Print or further analyze the rows containing outliers\n",
    "print(f\"Rows containing outliers in '{column_to_check}':\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=conventional_total_us_df[column_to_check])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organic avocados in whole US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = organic_total_us_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate the Z-scores for the numeric columns\n",
    "z_scores = np.abs(stats.zscore(numeric_columns))\n",
    "\n",
    "# Define the threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# Create a mask to identify outlier rows\n",
    "outlier_mask = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "# Get the rows containing outliers\n",
    "outliers = organic_total_us_df[outlier_mask]\n",
    "\n",
    "# Print or further analyze the rows containing outliers\n",
    "print(\"Rows containing outliers:\")\n",
    "outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_check = 'AveragePrice'\n",
    "\n",
    "# Calculate the Z-scores for the selected columns\n",
    "z_scores = np.abs(stats.zscore(organic_total_us_df[column_to_check]))\n",
    "\n",
    "# Define the threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# Create a mask to identify outlier rows\n",
    "outlier_mask = z_scores > threshold\n",
    "\n",
    "# Get the rows containing outliers in the selected column\n",
    "outliers = organic_total_us_df[outlier_mask]\n",
    "\n",
    "# Print or further analyze the rows containing outliers\n",
    "print(f\"Rows containing outliers in '{column_to_check}':\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = np.percentile(organic_total_us_df[column_to_check], 25)\n",
    "Q3 = np.percentile(organic_total_us_df[column_to_check], 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for identifying outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame to get rows with outliers in the selected column\n",
    "outliers = organic_total_us_df[(organic_total_us_df[column_to_check] < lower_bound) | (organic_total_us_df[column_to_check] > upper_bound)]\n",
    "\n",
    "# Print or further analyze the rows containing outliers\n",
    "print(f\"Rows containing outliers in '{column_to_check}':\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=organic_total_us_df[column_to_check])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_check = 'Total Volume'\n",
    "\n",
    "# Calculate the Z-scores for the selected columns\n",
    "z_scores = np.abs(stats.zscore(organic_total_us_df[column_to_check]))\n",
    "\n",
    "# Define the threshold for identifying outliers\n",
    "threshold = 3\n",
    "\n",
    "# Create a mask to identify outlier rows\n",
    "outlier_mask = z_scores > threshold\n",
    "\n",
    "# Get the rows containing outliers in the selected column\n",
    "outliers = organic_total_us_df[outlier_mask]\n",
    "\n",
    "# Print or further analyze the rows containing outliers\n",
    "print(f\"Rows containing outliers in '{column_to_check}':\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = np.percentile(organic_total_us_df[column_to_check], 25)\n",
    "Q3 = np.percentile(organic_total_us_df[column_to_check], 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for identifying outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame to get rows with outliers in the selected column\n",
    "outliers = organic_total_us_df[(organic_total_us_df[column_to_check] < lower_bound) | (organic_total_us_df[column_to_check] > upper_bound)]\n",
    "\n",
    "# Print or further analyze the rows containing outliers\n",
    "print(f\"Rows containing outliers in '{column_to_check}':\")\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=organic_total_us_df[column_to_check])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data types - ensure columns have expected data types (string, numeric, datetime, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess missing data - check for missing/null values and determine strategy for handling them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the dataframe by the column *Date*\n",
    "\n",
    "The first step is to sort the dataframe by date in order to have the records in chronological order from earlier observation to latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avocado_df = avocado_df.sort_values(by='Date')\n",
    "\n",
    "sorted_avocado_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avocado_df['TotalSales'] = sorted_avocado_df['AveragePrice'] * sorted_avocado_df['Total Volume']\n",
    "sorted_avocado_df['TotalSales'] = sorted_avocado_df['TotalSales'].round(2)\n",
    "\n",
    "sorted_avocado_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = sorted_avocado_df.dtypes\n",
    "print(column_types)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see on this dataset there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(sorted_avocado_df)\n",
    "\n",
    "for column in columns:\n",
    "    col = Column(column, sorted_avocado_df[column])\n",
    "    \n",
    "    print('Column Name : ', col.getName())\n",
    "    print('Column Null Contents : ', col.getNullContents())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of avocados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avocado_df['type'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_avocado_df['Date'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis per region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create region objects and append them to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionValueCounts=sorted_avocado_df['region'].value_counts()\n",
    "\n",
    "regions={}\n",
    "\n",
    "for name, volume in regionValueCounts.items():\n",
    "    region=Region(name,volume, sorted_avocado_df)\n",
    "    regions[region.getName()]=region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the details of first region and the head of its dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions['Southeast'].details()\n",
    "regions['Southeast'].getHeadOfDataframe(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions['Southeast'].PrintStatisticsOfColumns()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print plots and statistics of all the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print plots and statistics of the top 3 regions in sales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will exclude the TotalUS as region to find the top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_sales = sorted_avocado_df[sorted_avocado_df['region'] != 'TotalUS'].groupby('region')['TotalSales'].sum().reset_index()\n",
    "\n",
    "# Sort the regions based on total sales in descending order\n",
    "sorted_regions = region_sales.sort_values(by='TotalSales', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Select the top 3 regions with the highest total sales\n",
    "top_3_regions = sorted_regions.head(3)\n",
    "\n",
    "print(top_3_regions['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, region in regions.items():\n",
    "    if name == top_3_regions['region'][0] or name == top_3_regions['region'][1] or name == top_3_regions['region'][2]:\n",
    "        print(name)\n",
    "        region.plotColumnAndPrintStatisticsOfIt('TotalSales')\n",
    "        print(\"=\"*20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print plots and statistics of the top 3 regions in Average Price"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will exclude the TotalUS as region to find the top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_sales = sorted_avocado_df[sorted_avocado_df['region'] != 'TotalUS'].groupby('region')['AveragePrice'].mean().reset_index()\n",
    "\n",
    "# Sort the regions based on total sales in descending order\n",
    "sorted_regions = region_sales.sort_values(by='AveragePrice', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Select the top 3 regions with the highest total sales\n",
    "top_3_regions = sorted_regions.head(3)\n",
    "\n",
    "print(top_3_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, region in regions.items():\n",
    "    if name == top_3_regions['region'][0] or name == top_3_regions['region'][1] or name == top_3_regions['region'][2]:\n",
    "        print(name)\n",
    "        region.plotColumnAndPrintStatisticsOfIt('AveragePrice')\n",
    "        print(\"=\"*20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organic type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organic_df=sorted_avocado_df[sorted_avocado_df.type=='organic']\n",
    "\n",
    "organic_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly patterns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Prophet model\n",
    "model = Prophet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "df = region.getConventionalDataframe()[['Date', 'AveragePrice']]\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.sort_values(by='Date', inplace=True)\n",
    "df.columns = ['ds', 'y']\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df =df[:train_size]\n",
    "test_df = df[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "future = model.make_future_dataframe(periods=26, freq='W')\n",
    "forecast = model.predict(future)\n",
    "\n",
    "fcst = model.predict(future)\n",
    "fig = model.plot(fcst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "model.plot(forecast, ax=ax)\n",
    "ax.plot(test_df.ds, test_df.y, color='black', label='Actual')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast Average Prices for regions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following columns we will try to forecast on conventional and organic data the future."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('cmdstanpy')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "cap = 2.25\n",
    "floor = 0.5\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for name, region in regions.items():\n",
    "    print(region.getName())\n",
    "    \n",
    "    # Create a Prophet model\n",
    "    model = Prophet(growth = 'logistic')\n",
    "    \n",
    "    # Preprocess the data\n",
    "    df = region.getConventionalDataframe()[['Date', 'AveragePrice']]\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.sort_values(by='Date', inplace=True)\n",
    "    df.columns = ['ds', 'y']\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "    df['cap'] = cap\n",
    "    df['floor'] = floor\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    train_df =df[:train_size]\n",
    "    test_df = df[train_size:]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(train_df)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    future = model.make_future_dataframe(periods=len(test_df), freq='W')\n",
    "\n",
    "    future['cap']=cap\n",
    "    future['floor']=floor\n",
    "\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    fcst = model.predict(future)\n",
    "\n",
    "    # fig = model.plot(fcst)\n",
    "\n",
    "    # Visualize the results\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    model.plot(fcst, ax=ax)\n",
    "    ax.plot(test_df.ds, test_df.y, color='black', label='Actual')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    # Plot the forecast without showing warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('cmdstanpy')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "cap = 2.25\n",
    "floor = 0.5\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "region=regions['TotalUS']\n",
    "print(region.getName())\n",
    "\n",
    "# Create a Prophet model\n",
    "model = Prophet(growth = 'logistic')\n",
    "\n",
    "# Preprocess the data\n",
    "df = region.getConventionalDataframe()[['Date', 'AveragePrice']]\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.sort_values(by='Date', inplace=True)\n",
    "df.columns = ['ds', 'y']\n",
    "df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "df['cap'] = cap\n",
    "df['floor'] = floor\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df =df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "future = model.make_future_dataframe(periods=len(test_df), freq='W')\n",
    "\n",
    "future['cap']=cap\n",
    "future['floor']=floor\n",
    "\n",
    "forecast = model.predict(future)\n",
    "\n",
    "fcst = model.predict(future)\n",
    "\n",
    "# fig = model.plot(fcst)\n",
    "\n",
    "# Visualize the results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "model.plot(fcst, ax=ax)\n",
    "ax.plot(test_df.ds, test_df.y, color='black', label='Actual')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# Plot the forecast without showing warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('cmdstanpy')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "cap = 2.75\n",
    "floor = 0.5\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for name, region in regions.items():\n",
    "    print(region.getName())\n",
    "    \n",
    "    # Create a Prophet model\n",
    "    model = Prophet(growth = 'logistic')\n",
    "    \n",
    "    # Preprocess the data\n",
    "    df = region.getOrganicDataframe()[['Date', 'AveragePrice']]\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.sort_values(by='Date', inplace=True)\n",
    "    df.columns = ['ds', 'y']\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "    df['cap'] = cap\n",
    "    df['floor'] = floor\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    train_df =df[:train_size]\n",
    "    test_df = df[train_size:]\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(train_df)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    future = model.make_future_dataframe(periods=len(test_df), freq='W')\n",
    "\n",
    "    future['cap']=cap\n",
    "    future['floor']=floor\n",
    "\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    fcst = model.predict(future)\n",
    "\n",
    "    # fig = model.plot(fcst)\n",
    "\n",
    "    # Visualize the results\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    model.plot(fcst, ax=ax)\n",
    "    ax.plot(test_df.ds, test_df.y, color='black', label='Actual')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    # Plot the forecast without showing warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
